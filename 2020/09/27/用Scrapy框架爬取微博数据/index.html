<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>用Scrapy框架爬取微博数据 | 左菲的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="[TOC] 一、Scrapy框架目录结构如下： 12345678+--weiboScrapy|     +--spider|      |    +--weibo.py|     +--items.py|     +--middlewares.py|     +--pipelines.py|     +--settings.py+--scrapy.cfg爬取思路：以微博的几个大V为起点，爬取他们的">
<meta property="og:type" content="article">
<meta property="og:title" content="用Scrapy框架爬取微博数据">
<meta property="og:url" content="http://example.com/2020/09/27/%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE/index.html">
<meta property="og:site_name" content="左菲的博客">
<meta property="og:description" content="[TOC] 一、Scrapy框架目录结构如下： 12345678+--weiboScrapy|     +--spider|      |    +--weibo.py|     +--items.py|     +--middlewares.py|     +--pipelines.py|     +--settings.py+--scrapy.cfg爬取思路：以微博的几个大V为起点，爬取他们的">
<meta property="og:locale">
<meta property="article:published_time" content="2020-09-27T08:09:26.000Z">
<meta property="article:modified_time" content="2020-09-27T08:38:25.459Z">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="左菲的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">左菲的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-用Scrapy框架爬取微博数据" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/27/%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE/" class="article-date">
  <time datetime="2020-09-27T08:09:26.000Z" itemprop="datePublished">2020-09-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      用Scrapy框架爬取微博数据
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="一、Scrapy框架"><a href="#一、Scrapy框架" class="headerlink" title="一、Scrapy框架"></a>一、Scrapy框架</h1><p>目录结构如下：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+--weiboScrapy</span><br><span class="line">|     +--spider</span><br><span class="line">|      |    +--weibo.py</span><br><span class="line">|     +--items.py</span><br><span class="line">|     +--middlewares.py</span><br><span class="line">|     +--pipelines.py</span><br><span class="line">|     +--settings.py</span><br><span class="line">+--scrapy.cfg</span><br></pre></td></tr></table></figure><br><strong>爬取思路</strong>：以微博的几个大V为起点，爬取他们的用户详情信息、关注列表、粉丝列表、微博列表，然后通过粉丝列表、关注列表爬取各个粉丝、被关注者的信息，以此类推，形成递归爬取。如果一个用户和其他用户在社交网络上有关联，我们就能爬取到他的信息，这样我们就能爬取到所有用户的信息。</p>
<h2 id="1-weibo-py"><a href="#1-weibo-py" class="headerlink" title="1.weibo.py"></a>1.weibo.py</h2><p>| 爬虫类，提取数据项填充Item</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class WeibocomSpider(Spider):</span><br><span class="line">    </span><br><span class="line">    def start_requests(self):</span><br><span class="line">        #依次抓取各个大V的个人详情，使用parse_user作为回调函数</span><br><span class="line">    def parse_user(self, response):</span><br><span class="line">        #解析用户信息，并生成 UserItem 返回</span><br><span class="line">        #借由response.url中的page_id，构造访问关注列表、粉丝列表、微博列表的Resquest（使用yield）</span><br><span class="line">    </span><br><span class="line">    def getpage_id(self,uid):</span><br><span class="line">        #根据uid(十位)访问获取domid，拼成能访问用户详情页的page_id(十六位)</span><br><span class="line">    def parse_follows(self, response):</span><br><span class="line">        #解析用户关注列表，填充UserRelationItem；解析各被关注者详情页信息；生成访问下一页列表的Request</span><br><span class="line">    def parse_fans(self, response):</span><br><span class="line">        #解析用户粉丝，填充UserRelationItem；解析各粉丝详情页信息；生成访问下一页列表的Request</span><br><span class="line">    </span><br><span class="line">    def parse_weibos(self, response):</span><br><span class="line">        &quot;#解析微博列表,填充WeiboItem;生成访问下一页微博的Request</span><br></pre></td></tr></table></figure>

<h2 id="2-items-py"><a href="#2-items-py" class="headerlink" title="2.items.py"></a>2.items.py</h2><p>| 定义需要提取的数据项</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class UserItem(Item):</span><br><span class="line">class UserRelationItem(Item):</span><br><span class="line">class WeiboItem(Item):</span><br></pre></td></tr></table></figure>

<h2 id="3-middlewares-py"><a href="#3-middlewares-py" class="headerlink" title="3.middlewares.py"></a>3.middlewares.py</h2><p>| 相比于Spider Middlewares，更常用的是Downloader Middleswares。Downloader将接收Request并下载得到Response，来回都会经过Downloader，因此Downloader Middleswares可以用来执行各种破解反爬虫的策略，如对接Cookies池、代理池。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class ProxyMiddleware():</span><br><span class="line">class CookiesMiddleware():</span><br><span class="line">    def __init__(self,host,port,password):</span><br><span class="line">    </span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line"></span><br><span class="line">    def get_random_cookies(self):</span><br><span class="line">        #Redis数据库，获取cookies</span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        #通过get_random_cookies获取cookies，将str类型的cookies转为JSON类型，赋给request.cookies</span><br></pre></td></tr></table></figure>

<h2 id="4-pipeline-py"><a href="#4-pipeline-py" class="headerlink" title="4.pipeline.py"></a>4.pipeline.py</h2><p>| 对提取到的数据项Items进行清洗、检查、去重和保存</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class MongoPipeline(object):</span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">    </span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        #初始化MongoDB，为两个Collection(users、weibos)创建索引，索引字段是id</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        #如果是UserItem和WeiboItem则更新相应的Collection，如果是UserRelationItem则向users Collection插入粉丝列表和关注列表</span><br></pre></td></tr></table></figure>

<h1 id="二、代码实现过程"><a href="#二、代码实现过程" class="headerlink" title="二、代码实现过程"></a>二、代码实现过程</h1><h2 id="1-获取访问接口"><a href="#1-获取访问接口" class="headerlink" title="1.获取访问接口"></a>1.获取访问接口</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#用户信息API</span><br><span class="line">   user_url &#x3D; &#39;https:&#x2F;&#x2F;weibo.com&#x2F;p&#x2F;&#123;page_id&#125;&#x2F;info?mod&#x3D;pedit_more&#39;</span><br><span class="line">   #关注列表API</span><br><span class="line">   follow_url &#x3D; &#39;https:&#x2F;&#x2F;weibo.com&#x2F;p&#x2F;&#123;page_id&#125;&#x2F;follow?page&#x3D;&#123;page&#125;&#39;</span><br><span class="line">   #粉丝列表API</span><br><span class="line">   fan_url &#x3D; &#39;https:&#x2F;&#x2F;weibo.com&#x2F;p&#x2F;&#123;page_id&#125;&#x2F;follow?relate&#x3D;fans&amp;page&#x3D;&#123;page&#125;&#39;</span><br><span class="line">   #微博信息API</span><br><span class="line">   weibo_url &#x3D; &#39;https:&#x2F;&#x2F;weibo.com&#x2F;p&#x2F;&#123;page_id&#125;&#x2F;home?page&#x3D;&#123;page&#125;&#39;</span><br></pre></td></tr></table></figure>

<h2 id="2-定义数据项"><a href="#2-定义数据项" class="headerlink" title="2.定义数据项"></a>2.定义数据项</h2><p>困难1：用户信息的UserItem不好定义。（1）微博在个人详情页供人填写的信息有很多，有十几二十项，但用户只会填写其中几项，很分散。（2）有些微博信息有一些是供人自定义填写的，比如标签信息，还有教育信息，用户可以选择填写零到多项。<br>我的解决办法是，尽量找到用户都会填写的信息，比如描述、所在地、简介、性别，舍去那些用户不常填写的数据项<br><em>微博上可以填写的信息：<br>基本信息：昵称，真实姓名，所在地，性别，性取向，感情状况，生日，血型，博客地址，个性域名，简介，注册时间<br>联系信息：邮箱，QQ，MSN<br>职业信息：公司，地区，职位<br>教育信息：大学，高职，高中，中专技校，初中，小学，海外<br>标签信息：</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">class UserItem(Item):</span><br><span class="line">    collection &#x3D; &#39;users&#39;#指明保存的Collection的名称</span><br><span class="line">    </span><br><span class="line">    page_id &#x3D; Field()#用户ID</span><br><span class="line">    nikename &#x3D; Field()#昵称</span><br><span class="line">    brief_introduction &#x3D; Field()#简介</span><br><span class="line">    loaction &#x3D; Field()#所在地</span><br><span class="line">    registered_time &#x3D; Field()#注册时间</span><br><span class="line">    </span><br><span class="line">    fans_count &#x3D; Field()#粉丝数量</span><br><span class="line">    follows_count &#x3D; Field()#关注数量</span><br><span class="line">    weibos_count &#x3D; Field()#微博数量</span><br><span class="line">    </span><br><span class="line">    follows &#x3D; Field()#用户关注列表</span><br><span class="line">    fans &#x3D; Field()#粉丝列表</span><br><span class="line">    crawled_at &#x3D; Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class UserRelationItem(Item):</span><br><span class="line">    #用户的关注和粉丝列表直接定义为、单独的 UserRelationitem，</span><br><span class="line">    #但会和并存储到到用户的Collection里</span><br><span class="line">    collection &#x3D; &#39;users&#39;</span><br><span class="line">    </span><br><span class="line">    page_id &#x3D; Field()#用户的page_id</span><br><span class="line">    follows &#x3D; Field()#用户关注列表</span><br><span class="line">    fans &#x3D; Field()#粉丝列表</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class WeiboItem(Item):</span><br><span class="line">    collection &#x3D; &#39;weibos&#39;</span><br><span class="line">    </span><br><span class="line">    page_id &#x3D; Field()</span><br><span class="line">    forward_num &#x3D; Field()</span><br><span class="line">    comment_num &#x3D; Field()</span><br><span class="line">    likes_num &#x3D; Field()</span><br><span class="line">    text &#x3D; Field()</span><br><span class="line">    image_src &#x3D; Field()</span><br><span class="line">    time &#x3D; Field()</span><br><span class="line">    phone &#x3D; Field()</span><br></pre></td></tr></table></figure>

<h2 id="3-解析数据"><a href="#3-解析数据" class="headerlink" title="3.解析数据"></a>3.解析数据</h2><h3 id="3-1解析用户信息，并生成-UserItem-返回"><a href="#3-1解析用户信息，并生成-UserItem-返回" class="headerlink" title="3.1解析用户信息，并生成 UserItem 返回"></a>3.1解析用户信息，并生成 UserItem 返回</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">user_item &#x3D; UserItem()</span><br><span class="line">try:</span><br><span class="line">    result &#x3D; re.search(r&#39;domid\&quot;:\&quot;Pl_Official_PersonalInfo__.*?html\&quot;:\&quot;(.*?)\&quot;&#125;&#39;,response.text).group(1)#提取出&lt;script&gt;中的html字符串</span><br><span class="line">    user_item[&#39;page_id&#39;] &#x3D; re.search(r&#39;p&#x2F;(\d*)&#x2F;info&#39;,response.url).group(1)</span><br><span class="line">    user_item[&#39;nikename&#39;]&#x3D;re.search(r&#39;[昵][称].*?pt_detail\\&quot;&gt;(.*?)&lt;&#39;,result).group(1)</span><br><span class="line">    user_item[&#39;brief_introduction&#39;]&#x3D;re.search(r&#39;[简][介].*?pt_detail\\&quot;&gt;(.*?)&lt;&#39;,result).group(1)</span><br><span class="line">    user_item[&#39;loaction&#39;]&#x3D;re.search(r&#39;[所][在][地].*?pt_detail\\&quot;&gt;(.*?)&lt;&#39;,result).group(1)</span><br><span class="line">    user_item[&#39;registered_time&#39;]&#x3D;re.search(r&#39;[注][册][时][间].*?pt_detail\\&quot;&gt;(.*?)&lt;&#39;,result).group(1)</span><br><span class="line"></span><br><span class="line">    num_info&#x3D;re.search(r&#39;domid&quot;:&quot;Pl_Core_T8CustomTriColumn__.*?html&quot;:&quot;(.*?)&quot;&#125;&#39;,response.text).group(1)#提取出含有粉丝数&#x2F;微博数&#x2F;关注数信息的html</span><br><span class="line">    strlist&#x3D;[]</span><br><span class="line">    for match in re.finditer(r&#39;W_f12\\&quot;&gt;(\d*)&lt;&#39;,num_info):#返回一个迭代类型       </span><br><span class="line">        strlist.append(match.group(1))</span><br><span class="line">    user_item[&#39;fans_count&#39;]&#x3D;strlist[0]</span><br><span class="line">    user_item[&#39;follows_count&#39;]&#x3D;strlist[1]</span><br><span class="line">    user_item[&#39;weibos_count&#39;]&#x3D;strlist[2]</span><br><span class="line">except:</span><br><span class="line">   print(&quot;解析用户信息出错!&quot;)</span><br><span class="line">yield user_item</span><br></pre></td></tr></table></figure>

<h3 id="3-2解析用户关注列表，填充UserRelationItem"><a href="#3-2解析用户关注列表，填充UserRelationItem" class="headerlink" title="3.2解析用户关注列表，填充UserRelationItem"></a>3.2解析用户关注列表，填充UserRelationItem</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">result &#x3D; re.search(r&#39;pl.content.followTab.index.*?html&quot;:&quot;(.*?)&quot;&#125;&#39;,response.text).group(1) #提取出&lt;script&gt;中的html字符串</span><br><span class="line">html&#x3D;etree.HTML(result.replace(&#39;\\&#39;,&#39;&#39;)) #将转义符删除，否则xpath选择器会解析不出结果（一些转义字符也会显示出正常效果，如</span><br><span class="line">&quot;&amp;&quot;显示为&quot;&amp;&quot;）</span><br><span class="line">follows&#x3D;html.xpath(&#39;&#x2F;&#x2F;ul[@class&#x3D;&quot;follow_list&quot;]&#x2F;li[@class&#x3D;&quot;follow_item S_line2&quot;]&#39;) </span><br><span class="line">page_id&#x3D;re.search(r&quot;\$CONFIG\[&#39;page_id&#39;\]&#x3D;&#39;(\d&#123;16&#125;)&#39;;&quot;,response.text).group(1)</span><br><span class="line">#关注列表</span><br><span class="line">user_relation_item &#x3D; UserRelationItem()</span><br><span class="line">follows&#x3D;[&#123;&#39;uid&#39;:re.search(r&#39;uid&#x3D;(\d&#123;10&#125;)&amp;&#39;,follow.xpath(&#39;.&#x2F;&#x2F;@action-data&#39;)[0]).group(1),</span><br><span class="line">          &#39;name&#39;:re.search(r&#39;fnick&#x3D;(.*?)&amp;&#39;,follow.xpath(&#39;.&#x2F;&#x2F;@action-data&#39;)[0]).group(1)&#125; for follow in follows]#注意&quot;&#x2F;&#x2F;&quot;，</span><br><span class="line">user_relation_item[&#39;page_id&#39;]&#x3D;page_id</span><br><span class="line">user_relation_item[&#39;follows&#39;]&#x3D;follows</span><br><span class="line">user_relation_item[&#39;fans&#39;]&#x3D;[]</span><br><span class="line">yield user_relation_item</span><br><span class="line">&#96;&#96;&#96; </span><br><span class="line"></span><br><span class="line">### 3.3解析用户粉丝，填充UserRelationItem</span><br></pre></td></tr></table></figure>
<p>result = re.search(r’pl.content.followTab.index.<em>?html”:”(.</em>?)”}’,response.text).group(1)<br>html=etree.HTML(result.replace(‘\‘,’’))<br>fans=html.xpath(‘//ul[@class=”follow_list”]/li[@class=”follow_item S_line2”]’)<br>page_id=re.search(r”$CONFIG[‘page_id’]=’(\d{16})’;”,response.text).group(1)#用户的page_id<br>#粉丝列表<br>user_relation_item = UserRelationItem()<br>fans=[{‘uid’:re.search(r’uid=(\d{10})&amp;’,fan.xpath(‘.//@action-data’)[0]).group(1),<br>          ‘name’:re.search(r’fnick=(.*?)&amp;’,fan.xpath(‘.//@action-data’)[0]).group(1)} for fan in fans]<br>user_relation_item[‘page_id’]=page_id<br>user_relation_item[‘follows’]=[]<br>user_relation_item[‘fans’]=fans<br>yield user_relation_item</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 3.4解析微博列表,填充WeiboItem</span><br></pre></td></tr></table></figure>
<p>result = re.search(r’pl.content.homeFeed.index.<em>?html”:”(.</em>?)”}’,response.text).group(1)<br>html=etree.HTML(result.replace(‘\‘,’’))<br>weibos=html.xpath(‘//div[@action-data=”cur_visible=0”]’)<br>for weibo in weibos:<br>    weibo_item=WeiboItem()<br>    weibo_item[‘page_id’]=re.search(r’p/(\d{16})/home?’,response.url).group(1)<br>    weibo_item[‘forward_num’]=weibo.xpath(‘.//span[contains(@class,”line”)]//em[last()]/text()’)[1]<br>    weibo_item[‘comment_num’]=weibo.xpath(‘.//span[contains(@class,”line”)]//em[last()]/text()’)[2]<br>    weibo_item[‘likes_num’]=weibo.xpath(‘.//span[contains(@class,”line”)]//em[last()]/text()’)[3]<br>    weibo_item[‘text’]=’ ‘.join(weibo.xpath(‘.//div[@class=”WB_text”]/text()’))<br>    weibo_item[‘image_src’]=weibo.xpath(‘.//div[@class=”media_box”]//img/@src’)<br>    weibo_item[‘time’]=weibo.xpath(‘.//div[contains(@class,”WB_from”)]/a[position()=1]/text()’)[0]<br>    weibo_item[‘phone’]=weibo.xpath(‘.//div[contains(@class,”WB_from”)]/a[position()=2]/text()’)[0]<br>    yield weibo_item</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**提示：可以将网页的HTML保存进项目目录，正则表达式测试工具、XPath测试工具来测试所编写的选择器，甚至可以写一段测试代码，查看选择器返回后的数据及其类型**</span><br><span class="line"></span><br><span class="line">## 4.实现递归的方式</span><br></pre></td></tr></table></figure>
<p>class WeibocomSpider(Spider):<br>    #选取几个微博大V的列表<br>    start_users = [‘1003062687827715’, ‘1004061797270765’, ‘1005051046193337’, ‘1004061241148864’]</p>
<pre><code>def start_requests(self):
    #依次抓取各个大V的个人详情，使用parse_user作为回调函数
    for page_id in self.start_users:
        yield Request(self.user_url.format(page_id=page_id), callback=self.parse_user)

def parse_user(self, response):
    #借由response.url中的page_id，构造访问关注列表、粉丝列表、微博列表的Resquest
    # 关注
    page_id = re.search(r&#39;p/(\d*)/info&#39;,response.url).group(1)
    yield Request(self.follow_url.format(page_id=page_id, page=1), callback=self.parse_follows,
                      meta=&#123;&#39;page&#39;: 1, &#39;page_id&#39;: page_id&#125;)
    # 粉丝
    yield Request(self.fan_url.format(page_id=page_id, page=1), callback=self.parse_fans,
                      meta=&#123;&#39;page&#39;: 1, &#39;page_id&#39;: page_id&#125;)
    # 微博
    yield Request(self.weibo_url.format(page_id=page_id, page=1), callback=self.parse_weibos,
                      meta=&#123;&#39;page&#39;: 1, &#39;page_id&#39;: page_id&#125;)

def getpage_id(self,uid):
    #根据uid(十位)访问获取domid，拼成能访问用户详情页的page_id(十六位)
    response=requests.get(&#39;https://weibo.com/u/&#123;uid&#125;&#39;.format(uid=uid))
    return re.search(r&quot;$CONFIG[&#39;page_id&#39;]=&#39;(\d&#123;16&#125;)&#39;;&quot;,response.text)

def parse_follows(self, response):
    #解析被关注的用户的信息
    for follow in follows:
        uid=re.search(r&#39;uid=(\d&#123;10&#125;)&amp;&#39;,follow.xpath(&#39;.//@action-data&#39;)[0]).group(1)
        page_id=self.getpage_id(uid)#被关注者的page_id
        yield Request(self.user_url.format(page_id=page_id),callback=self.parse_user)
    #下一页
    page=int(re.search(r&#39;page=(\d)&#39;,response.url).group(1))+1
    if page&lt;=5:
        yield Request(self.follow_url.format(page_id=page_id,page=page),
                callback=self.parse_follows, meta=&#123;&#39;page&#39;: page, &#39;page_id&#39;: page_id&#125;)

def parse_fans(self, response):
    #解析粉丝的用户信息
    for fan in fans:
        uid=re.search(r&#39;uid=(\d&#123;10&#125;)&amp;&#39;,fan.xpath(&#39;.//@action-data&#39;)[0]).group(1)
        page_id=self.getpage_id(uid)#被关注者的page_id
        yield Request(self.user_url.format(page_id=page_id),callback=self.parse_user)
    #下一页粉丝
    page=int(re.search(r&#39;page=(\d)&#39;,response.url).group(1))+1
    if page&lt;=5:
        yield Request(self.fan_url.format(page_id=page_id,page=page),
                callback=self.parse_fans, meta=&#123;&#39;page&#39;: page, &#39;page_id&#39;: page_id&#125;)

def parse_weibos(self, response):
    #下一页微博
    page_id=re.search(r&#39;p/(\d&#123;16&#125;)/home?&#39;,response.url).group(1)
    page=int(re.search(r&#39;page=(\d)&#39;,response.url).gruop(1))+1
    yield Request(self.weibo_url.format(page_id=page_id, page=page), callback=self.parse_weibos,
                      meta=&#123;&#39;page_id&#39;: page_id, &#39;page&#39;: page&#125;)</code></pre>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">通过访问微博大V的用户详情页，生成初始Request并用parse_user作为回调函数。parse_user解析用户详情页，通过response.url中的page_id构造访问关注列表、粉丝列表、微博列表的Resquest。在parse_fans、parse_follows、parse_weibos中，通过response.url中的page_id和page构造下一页的Request。</span><br><span class="line"></span><br><span class="line">## 5.将Items保存到MongoDB</span><br></pre></td></tr></table></figure>
<p>class MongoPipeline(object):<br>    def <strong>init</strong>(self, mongo_uri, mongo_db):<br>        self.mongo_uri = mongo_uri<br>        self.mongo_db = mongo_db</p>
<pre><code>@classmethod
def from_crawler(cls, crawler):
    return cls(
        mongo_uri=crawler.settings.get(&#39;MONGO_URI&#39;),
        mongo_db=crawler.settings.get(&#39;MONGO_DATABASE&#39;)
    )

def open_spider(self, spider):
    self.client = pymongo.MongoClient(self.mongo_uri)
    self.db = self.client[self.mongo_db]
    #为两个Collection创建索引，索引字段是id
    self.db[UserItem.collection].create_index([(&#39;id&#39;, pymongo.ASCENDING)])
    self.db[WeiboItem.collection].create_index([(&#39;id&#39;, pymongo.ASCENDING)])

def close_spider(self, spider):
    self.client.close()

def process_item(self, item, spider):
    if isinstance(item, UserItem) or isinstance(item, WeiboItem):#isinstance判断两个类型是否相同
        self.db[item.collection].update(&#123;&#39;id&#39;:item.get(&#39;id&#39;)&#125;, &#123;&#39;$set&#39;:item&#125;, True)
        #$set 操作符，如果爬取到重复的数据即可对数据进行更新，同时不会删除已存在的字段 
        #参数设置为True ，如果数据不存在则插入数据。这样我们就可以做到数据存在即更新，数据不存在即插入，从而获得去重的效果
    if isinstance(item, UserRelationItem):
        self.db[item.collection].update(
            &#123;&#39;page_id&#39;: item.get(&#39;page_id&#39;)&#125;,
            &#123;&#39;$addToSet&#39;:#$addToSet操作符可以向列表类型的字段插入数据同时去重，它的值就是需要操作的字段名称
                &#123;
                    &#39;follows&#39;: &#123;&#39;$each&#39;: item[&#39;follows&#39;]&#125;,
                    #$each操作符对需要插入的列表数据进行了遍历，以逐条插入用户的关注或粉丝数据到指定的字段
                    &#39;fans&#39;: &#123;&#39;$each&#39;: item[&#39;fans&#39;]&#125;
                &#125;
            &#125;, True)
    return item</code></pre>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">最重要的是process_item方法，Item Pipeline会默认调用该方法对Items进行处理。该process_item方法所做的是，如果item是UserItem和WeiboItem则更新相应的Collection，如果是UserRelationItem则向users Collection插入粉丝列表和关注列表。</span><br><span class="line">如何对MongoDB进行操作，我有很多操作符都不是很懂，没见过也不会用。</span><br><span class="line"></span><br><span class="line">## 6.对接Cookies池</span><br><span class="line">如何用Python模拟登录微博，如何构建Cookies池，在别的博文里有提到。这里我们需要从Redis数据库中随机提取cookies，并将Request的cookies更换为Cookies池的cookies。</span><br></pre></td></tr></table></figure>
<p>class CookiesMiddleware():<br>    def <strong>init</strong>(self,host,port,password):<br>        self.logger = logging.getLogger(<strong>name</strong>)<br>        self.cookies_db = redis.StrictRedis(host=host, port=port, password=password, decode_responses=True)</p>
<pre><code>@classmethod
def from_crawler(cls, crawler):
    settings = crawler.settings
    return cls(
            host = settings.get(&#39;REDIS_HOST&#39;),
            port = settings.get(&#39;REDIS_PORT&#39;),
            password = settings.get(&#39;REDIS_PASSWORD&#39;)
           )

def get_random_cookies(self):
    cookies = self.cookies_db.hvals(&#39;cookies:weibo&#39;)
    return random.choice(cookies)

def process_request(self, request, spider):
    self.logger.debug(&#39;正在获取Cookies&#39;)
    cookies = self.get_random_cookies()#获取到的cookies为字符串类型
    cookies = json.loads(cookies)
    if cookies:
        request.cookies = cookies
        self.logger.debug(&#39;使用Cookies &#39; , cookies)</code></pre>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">process_request是Downloader Middleware的核心方法之一，另外两个是process_response和process_exception。</span><br><span class="line"></span><br><span class="line">## 7.settings.py打开pipeline和middleware</span><br></pre></td></tr></table></figure>
<p>DOWNLOADER_MIDDLEWARES = {<br>    #’weiboScrapy.middlewares.WeiboscrapyDownloaderMiddleware’: 543,<br>    ‘weiboScrapy.middlewares.CookiesMiddleware’: 550,<br>    #’weiboScrapy.middlewares.ProxyMiddleware’: 555,<br>}<br>ITEM_PIPELINES = {<br>    #’weiboScrapy.pipelines.WeiboscrapyPipeline’: 300,<br>    #’weiboScrapy.pipelines.WeiboPipeline’: 301,<br>    ‘weiboScrapy.pipelines.MongoPipeline’: 302,<br>}</p>
<p>MONGO_URI = ‘localhost’<br>MONGO_DATABASE = ‘weibo’</p>
<p>REDIS_HOST = ‘127.0.0.1’<br>REDIS_PORT = ‘6379’<br>REDIS_PASSWORD = None</p>
<p>```</p>
<h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><p><strong>爬虫时遇到的难点：</strong></p>
<ol>
<li>必须要登陆才能看到关注信息；</li>
<li>只可以查看前五页的粉丝列表关注者列表；</li>
<li>用户主页微博列表是动态生成的，那些动态加载的微博在Network中再搜索不出，但是划到最后是可以找到分页的。</li>
<li>URL所用到的page_id要比uid多几位数字，如100406、100505、100306，即“domid”，应该是用来区分用户类型的。如果URL中用错了或者省略domid，会被重定向到主页。因此在提取关注者、粉丝信息时，还需要知道domid。</li>
<li>提取微博文章时，如果文本过长会被折叠，显示“展开全文”链接才会显示全文。点赞数、转发数、评论数是实时变动的，从网页中提取到的永远会滞后。</li>
<li>登录时，账户默认开启了登录保护（新注册的账号也一样），要求你进行短信验证或私信验证。</li>
</ol>
<p><strong>解决策略：</strong><br>（1）微博模拟登陆，构建Cookies池<br>（2）由于是系统限制只能搜集前五页的粉丝列表和关注列表，因此就只能按照其规则来。<br>（3）动态生成，是因为执行JavaScript后再向后台发送了Ajax请求，浏览器拿到数据后再进一步渲染出来的。所以我们需要用程序模拟这些Ajax请求，提取我们想要的数据。<br>（4）对于缺少domid的uid，可以先通过<a target="_blank" rel="noopener" href="http://weibo.com/u/%7Buid%7D%E8%AE%BF%E9%97%AE%E4%B8%BB%E9%A1%B5%EF%BC%8C%E4%BB%8E%E4%B8%BB%E9%A1%B5%E7%9A%84%E7%BD%91%E9%A1%B5%E4%B8%AD%E8%8E%B7%E5%8F%96domid%E3%80%82">http://weibo.com/u/{uid}访问主页，从主页的网页中获取domid。</a><br>（6）输入用户名和密码之后，会弹出短信验证或私信验证（默认开启的登录保护），有时又还会弹出验证码。短信验证和私信验证通过寻常的反爬虫手段无法解决（起码我的能力有限想不到解决办法），但是我找到了可以绕过登录保护的办法，就是先登录新浪邮箱，再从新浪邮箱跳转到微博，这样可以直接登录。**<em>我用来做爬虫的微博账号都是通过新浪邮箱注册的，所以这种方法仅针对通过新浪邮箱的账号和密码来登录微博的情况**</em></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/09/27/%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE/" data-id="ckfkuqb9d0001xou7equ6hn3o" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/09/27/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/09/27/%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE/">用Scrapy框架爬取微博数据</a>
          </li>
        
          <li>
            <a href="/2020/09/27/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 左菲的博客<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>
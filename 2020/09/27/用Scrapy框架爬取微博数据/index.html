<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>用Scrapy框架爬取微博数据 | 左菲的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="[TOC] 一、Scrapy框架目录结构如下： 12345678+--weiboScrapy|     +--spider|      |    +--weibo.py|     +--items.py|     +--middlewares.py|     +--pipelines.py|     +--settings.py+--scrapy.cfg爬取思路：以微博的几个大V为起点，爬取他们的">
<meta property="og:type" content="article">
<meta property="og:title" content="用Scrapy框架爬取微博数据">
<meta property="og:url" content="http://example.com/2020/09/27/%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE/index.html">
<meta property="og:site_name" content="左菲的博客">
<meta property="og:description" content="[TOC] 一、Scrapy框架目录结构如下： 12345678+--weiboScrapy|     +--spider|      |    +--weibo.py|     +--items.py|     +--middlewares.py|     +--pipelines.py|     +--settings.py+--scrapy.cfg爬取思路：以微博的几个大V为起点，爬取他们的">
<meta property="og:locale">
<meta property="article:published_time" content="2020-09-27T08:09:26.000Z">
<meta property="article:modified_time" content="2020-09-27T08:45:09.321Z">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="左菲的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">左菲的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-用Scrapy框架爬取微博数据" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/27/%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE/" class="article-date">
  <time datetime="2020-09-27T08:09:26.000Z" itemprop="datePublished">2020-09-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      用Scrapy框架爬取微博数据
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="一、Scrapy框架"><a href="#一、Scrapy框架" class="headerlink" title="一、Scrapy框架"></a>一、Scrapy框架</h1><p>目录结构如下：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+--weiboScrapy</span><br><span class="line">|     +--spider</span><br><span class="line">|      |    +--weibo.py</span><br><span class="line">|     +--items.py</span><br><span class="line">|     +--middlewares.py</span><br><span class="line">|     +--pipelines.py</span><br><span class="line">|     +--settings.py</span><br><span class="line">+--scrapy.cfg</span><br></pre></td></tr></table></figure><br><strong>爬取思路</strong>：以微博的几个大V为起点，爬取他们的用户详情信息、关注列表、粉丝列表、微博列表，然后通过粉丝列表、关注列表爬取各个粉丝、被关注者的信息，以此类推，形成递归爬取。如果一个用户和其他用户在社交网络上有关联，我们就能爬取到他的信息，这样我们就能爬取到所有用户的信息。</p>
<h2 id="1-weibo-py"><a href="#1-weibo-py" class="headerlink" title="1.weibo.py"></a>1.weibo.py</h2><p>| 爬虫类，提取数据项填充Item</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class WeibocomSpider(Spider):</span><br><span class="line">    </span><br><span class="line">    def start_requests(self):</span><br><span class="line">        #依次抓取各个大V的个人详情，使用parse_user作为回调函数</span><br><span class="line">    def parse_user(self, response):</span><br><span class="line">        #解析用户信息，并生成 UserItem 返回</span><br><span class="line">        #借由response.url中的page_id，构造访问关注列表、粉丝列表、微博列表的Resquest（使用yield）</span><br><span class="line">    </span><br><span class="line">    def getpage_id(self,uid):</span><br><span class="line">        #根据uid(十位)访问获取domid，拼成能访问用户详情页的page_id(十六位)</span><br><span class="line">    def parse_follows(self, response):</span><br><span class="line">        #解析用户关注列表，填充UserRelationItem；解析各被关注者详情页信息；生成访问下一页列表的Request</span><br><span class="line">    def parse_fans(self, response):</span><br><span class="line">        #解析用户粉丝，填充UserRelationItem；解析各粉丝详情页信息；生成访问下一页列表的Request</span><br><span class="line">    </span><br><span class="line">    def parse_weibos(self, response):</span><br><span class="line">        &quot;#解析微博列表,填充WeiboItem;生成访问下一页微博的Request</span><br></pre></td></tr></table></figure>

<h2 id="2-items-py"><a href="#2-items-py" class="headerlink" title="2.items.py"></a>2.items.py</h2><p>| 定义需要提取的数据项</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class UserItem(Item):</span><br><span class="line">class UserRelationItem(Item):</span><br><span class="line">class WeiboItem(Item):</span><br></pre></td></tr></table></figure>

<h2 id="3-middlewares-py"><a href="#3-middlewares-py" class="headerlink" title="3.middlewares.py"></a>3.middlewares.py</h2><p>| 相比于Spider Middlewares，更常用的是Downloader Middleswares。Downloader将接收Request并下载得到Response，来回都会经过Downloader，因此Downloader Middleswares可以用来执行各种破解反爬虫的策略，如对接Cookies池、代理池。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class ProxyMiddleware():</span><br><span class="line">class CookiesMiddleware():</span><br><span class="line">    def __init__(self,host,port,password):</span><br><span class="line">    </span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line"></span><br><span class="line">    def get_random_cookies(self):</span><br><span class="line">        #Redis数据库，获取cookies</span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        #通过get_random_cookies获取cookies，将str类型的cookies转为JSON类型，赋给request.cookies</span><br></pre></td></tr></table></figure>

<h2 id="4-pipeline-py"><a href="#4-pipeline-py" class="headerlink" title="4.pipeline.py"></a>4.pipeline.py</h2><p>| 对提取到的数据项Items进行清洗、检查、去重和保存</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class MongoPipeline(object):</span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">    </span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        #初始化MongoDB，为两个Collection(users、weibos)创建索引，索引字段是id</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        #如果是UserItem和WeiboItem则更新相应的Collection，如果是UserRelationItem则向users Collection插入粉丝列表和关注列表</span><br></pre></td></tr></table></figure>

<h1 id="二、代码实现过程"><a href="#二、代码实现过程" class="headerlink" title="二、代码实现过程"></a>二、代码实现过程</h1><h2 id="1-获取访问接口"><a href="#1-获取访问接口" class="headerlink" title="1.获取访问接口"></a>1.获取访问接口</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#用户信息API</span><br><span class="line">   user_url &#x3D; &#39;https:&#x2F;&#x2F;weibo.com&#x2F;p&#x2F;&#123;page_id&#125;&#x2F;info?mod&#x3D;pedit_more&#39;</span><br><span class="line">   #关注列表API</span><br><span class="line">   follow_url &#x3D; &#39;https:&#x2F;&#x2F;weibo.com&#x2F;p&#x2F;&#123;page_id&#125;&#x2F;follow?page&#x3D;&#123;page&#125;&#39;</span><br><span class="line">   #粉丝列表API</span><br><span class="line">   fan_url &#x3D; &#39;https:&#x2F;&#x2F;weibo.com&#x2F;p&#x2F;&#123;page_id&#125;&#x2F;follow?relate&#x3D;fans&amp;page&#x3D;&#123;page&#125;&#39;</span><br><span class="line">   #微博信息API</span><br><span class="line">   weibo_url &#x3D; &#39;https:&#x2F;&#x2F;weibo.com&#x2F;p&#x2F;&#123;page_id&#125;&#x2F;home?page&#x3D;&#123;page&#125;&#39;</span><br></pre></td></tr></table></figure>

<h2 id="2-定义数据项"><a href="#2-定义数据项" class="headerlink" title="2.定义数据项"></a>2.定义数据项</h2><p>困难1：用户信息的UserItem不好定义。（1）微博在个人详情页供人填写的信息有很多，有十几二十项，但用户只会填写其中几项，很分散。（2）有些微博信息有一些是供人自定义填写的，比如标签信息，还有教育信息，用户可以选择填写零到多项。<br>我的解决办法是，尽量找到用户都会填写的信息，比如描述、所在地、简介、性别，舍去那些用户不常填写的数据项<br><em>微博上可以填写的信息：<br>基本信息：昵称，真实姓名，所在地，性别，性取向，感情状况，生日，血型，博客地址，个性域名，简介，注册时间<br>联系信息：邮箱，QQ，MSN<br>职业信息：公司，地区，职位<br>教育信息：大学，高职，高中，中专技校，初中，小学，海外<br>标签信息：</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">class UserItem(Item):</span><br><span class="line">    collection &#x3D; &#39;users&#39;#指明保存的Collection的名称</span><br><span class="line">    </span><br><span class="line">    page_id &#x3D; Field()#用户ID</span><br><span class="line">    nikename &#x3D; Field()#昵称</span><br><span class="line">    brief_introduction &#x3D; Field()#简介</span><br><span class="line">    loaction &#x3D; Field()#所在地</span><br><span class="line">    registered_time &#x3D; Field()#注册时间</span><br><span class="line">    </span><br><span class="line">    fans_count &#x3D; Field()#粉丝数量</span><br><span class="line">    follows_count &#x3D; Field()#关注数量</span><br><span class="line">    weibos_count &#x3D; Field()#微博数量</span><br><span class="line">    </span><br><span class="line">    follows &#x3D; Field()#用户关注列表</span><br><span class="line">    fans &#x3D; Field()#粉丝列表</span><br><span class="line">    crawled_at &#x3D; Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class UserRelationItem(Item):</span><br><span class="line">    #用户的关注和粉丝列表直接定义为、单独的 UserRelationitem，</span><br><span class="line">    #但会和并存储到到用户的Collection里</span><br><span class="line">    collection &#x3D; &#39;users&#39;</span><br><span class="line">    </span><br><span class="line">    page_id &#x3D; Field()#用户的page_id</span><br><span class="line">    follows &#x3D; Field()#用户关注列表</span><br><span class="line">    fans &#x3D; Field()#粉丝列表</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class WeiboItem(Item):</span><br><span class="line">    collection &#x3D; &#39;weibos&#39;</span><br><span class="line">    </span><br><span class="line">    page_id &#x3D; Field()</span><br><span class="line">    forward_num &#x3D; Field()</span><br><span class="line">    comment_num &#x3D; Field()</span><br><span class="line">    likes_num &#x3D; Field()</span><br><span class="line">    text &#x3D; Field()</span><br><span class="line">    image_src &#x3D; Field()</span><br><span class="line">    time &#x3D; Field()</span><br><span class="line">    phone &#x3D; Field()</span><br></pre></td></tr></table></figure>

<h2 id="3-解析数据"><a href="#3-解析数据" class="headerlink" title="3.解析数据"></a>3.解析数据</h2><h3 id="3-1解析用户信息，并生成-UserItem-返回"><a href="#3-1解析用户信息，并生成-UserItem-返回" class="headerlink" title="3.1解析用户信息，并生成 UserItem 返回"></a>3.1解析用户信息，并生成 UserItem 返回</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">user_item &#x3D; UserItem()</span><br><span class="line">try:</span><br><span class="line">    result &#x3D; re.search(r&#39;domid\&quot;:\&quot;Pl_Official_PersonalInfo__.*?html\&quot;:\&quot;(.*?)\&quot;&#125;&#39;,response.text).group(1)#提取出&lt;script&gt;中的html字符串</span><br><span class="line">    user_item[&#39;page_id&#39;] &#x3D; re.search(r&#39;p&#x2F;(\d*)&#x2F;info&#39;,response.url).group(1)</span><br><span class="line">    user_item[&#39;nikename&#39;]&#x3D;re.search(r&#39;[昵][称].*?pt_detail\\&quot;&gt;(.*?)&lt;&#39;,result).group(1)</span><br><span class="line">    user_item[&#39;brief_introduction&#39;]&#x3D;re.search(r&#39;[简][介].*?pt_detail\\&quot;&gt;(.*?)&lt;&#39;,result).group(1)</span><br><span class="line">    user_item[&#39;loaction&#39;]&#x3D;re.search(r&#39;[所][在][地].*?pt_detail\\&quot;&gt;(.*?)&lt;&#39;,result).group(1)</span><br><span class="line">    user_item[&#39;registered_time&#39;]&#x3D;re.search(r&#39;[注][册][时][间].*?pt_detail\\&quot;&gt;(.*?)&lt;&#39;,result).group(1)</span><br><span class="line"></span><br><span class="line">    num_info&#x3D;re.search(r&#39;domid&quot;:&quot;Pl_Core_T8CustomTriColumn__.*?html&quot;:&quot;(.*?)&quot;&#125;&#39;,response.text).group(1)#提取出含有粉丝数&#x2F;微博数&#x2F;关注数信息的html</span><br><span class="line">    strlist&#x3D;[]</span><br><span class="line">    for match in re.finditer(r&#39;W_f12\\&quot;&gt;(\d*)&lt;&#39;,num_info):#返回一个迭代类型       </span><br><span class="line">        strlist.append(match.group(1))</span><br><span class="line">    user_item[&#39;fans_count&#39;]&#x3D;strlist[0]</span><br><span class="line">    user_item[&#39;follows_count&#39;]&#x3D;strlist[1]</span><br><span class="line">    user_item[&#39;weibos_count&#39;]&#x3D;strlist[2]</span><br><span class="line">except:</span><br><span class="line">   print(&quot;解析用户信息出错!&quot;)</span><br><span class="line">yield user_item</span><br></pre></td></tr></table></figure>

<h3 id="3-2解析用户关注列表，填充UserRelationItem"><a href="#3-2解析用户关注列表，填充UserRelationItem" class="headerlink" title="3.2解析用户关注列表，填充UserRelationItem"></a>3.2解析用户关注列表，填充UserRelationItem</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">result &#x3D; re.search(r&#39;pl.content.followTab.index.*?html&quot;:&quot;(.*?)&quot;&#125;&#39;,response.text).group(1) #提取出&lt;script&gt;中的html字符串</span><br><span class="line">html&#x3D;etree.HTML(result.replace(&#39;\\&#39;,&#39;&#39;)) #将转义符删除，否则xpath选择器会解析不出结果（一些转义字符也会显示出正常效果，如</span><br><span class="line">&quot;&amp;&quot;显示为&quot;&amp;&quot;）</span><br><span class="line">follows&#x3D;html.xpath(&#39;&#x2F;&#x2F;ul[@class&#x3D;&quot;follow_list&quot;]&#x2F;li[@class&#x3D;&quot;follow_item S_line2&quot;]&#39;) </span><br><span class="line">page_id&#x3D;re.search(r&quot;\$CONFIG\[&#39;page_id&#39;\]&#x3D;&#39;(\d&#123;16&#125;)&#39;;&quot;,response.text).group(1)</span><br><span class="line">#关注列表</span><br><span class="line">user_relation_item &#x3D; UserRelationItem()</span><br><span class="line">follows&#x3D;[&#123;&#39;uid&#39;:re.search(r&#39;uid&#x3D;(\d&#123;10&#125;)&amp;&#39;,follow.xpath(&#39;.&#x2F;&#x2F;@action-data&#39;)[0]).group(1),</span><br><span class="line">          &#39;name&#39;:re.search(r&#39;fnick&#x3D;(.*?)&amp;&#39;,follow.xpath(&#39;.&#x2F;&#x2F;@action-data&#39;)[0]).group(1)&#125; for follow in follows]#注意&quot;&#x2F;&#x2F;&quot;，</span><br><span class="line">user_relation_item[&#39;page_id&#39;]&#x3D;page_id</span><br><span class="line">user_relation_item[&#39;follows&#39;]&#x3D;follows</span><br><span class="line">user_relation_item[&#39;fans&#39;]&#x3D;[]</span><br><span class="line">yield user_relation_item</span><br></pre></td></tr></table></figure>

<h3 id="3-3解析用户粉丝，填充UserRelationItem"><a href="#3-3解析用户粉丝，填充UserRelationItem" class="headerlink" title="3.3解析用户粉丝，填充UserRelationItem"></a>3.3解析用户粉丝，填充UserRelationItem</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">result &#x3D; re.search(r&#39;pl.content.followTab.index.*?html&quot;:&quot;(.*?)&quot;&#125;&#39;,response.text).group(1)</span><br><span class="line">html&#x3D;etree.HTML(result.replace(&#39;\\&#39;,&#39;&#39;))</span><br><span class="line">fans&#x3D;html.xpath(&#39;&#x2F;&#x2F;ul[@class&#x3D;&quot;follow_list&quot;]&#x2F;li[@class&#x3D;&quot;follow_item S_line2&quot;]&#39;)</span><br><span class="line">page_id&#x3D;re.search(r&quot;\$CONFIG\[&#39;page_id&#39;\]&#x3D;&#39;(\d&#123;16&#125;)&#39;;&quot;,response.text).group(1)#用户的page_id </span><br><span class="line">#粉丝列表</span><br><span class="line">user_relation_item &#x3D; UserRelationItem()</span><br><span class="line">fans&#x3D;[&#123;&#39;uid&#39;:re.search(r&#39;uid&#x3D;(\d&#123;10&#125;)&amp;&#39;,fan.xpath(&#39;.&#x2F;&#x2F;@action-data&#39;)[0]).group(1),</span><br><span class="line">          &#39;name&#39;:re.search(r&#39;fnick&#x3D;(.*?)&amp;&#39;,fan.xpath(&#39;.&#x2F;&#x2F;@action-data&#39;)[0]).group(1)&#125; for fan in fans]</span><br><span class="line">user_relation_item[&#39;page_id&#39;]&#x3D;page_id</span><br><span class="line">user_relation_item[&#39;follows&#39;]&#x3D;[]</span><br><span class="line">user_relation_item[&#39;fans&#39;]&#x3D;fans</span><br><span class="line">yield user_relation_item</span><br></pre></td></tr></table></figure>

<h3 id="3-4解析微博列表-填充WeiboItem"><a href="#3-4解析微博列表-填充WeiboItem" class="headerlink" title="3.4解析微博列表,填充WeiboItem"></a>3.4解析微博列表,填充WeiboItem</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">result &#x3D; re.search(r&#39;pl.content.homeFeed.index.*?html&quot;:&quot;(.*?)&quot;&#125;&#39;,response.text).group(1)</span><br><span class="line">html&#x3D;etree.HTML(result.replace(&#39;\\&#39;,&#39;&#39;))</span><br><span class="line">weibos&#x3D;html.xpath(&#39;&#x2F;&#x2F;div[@action-data&#x3D;&quot;cur_visible&#x3D;0&quot;]&#39;)</span><br><span class="line">for weibo in weibos:</span><br><span class="line">    weibo_item&#x3D;WeiboItem()</span><br><span class="line">    weibo_item[&#39;page_id&#39;]&#x3D;re.search(r&#39;p&#x2F;(\d&#123;16&#125;)&#x2F;home?&#39;,response.url).group(1)</span><br><span class="line">    weibo_item[&#39;forward_num&#39;]&#x3D;weibo.xpath(&#39;.&#x2F;&#x2F;span[contains(@class,&quot;line&quot;)]&#x2F;&#x2F;em[last()]&#x2F;text()&#39;)[1]</span><br><span class="line">    weibo_item[&#39;comment_num&#39;]&#x3D;weibo.xpath(&#39;.&#x2F;&#x2F;span[contains(@class,&quot;line&quot;)]&#x2F;&#x2F;em[last()]&#x2F;text()&#39;)[2]</span><br><span class="line">    weibo_item[&#39;likes_num&#39;]&#x3D;weibo.xpath(&#39;.&#x2F;&#x2F;span[contains(@class,&quot;line&quot;)]&#x2F;&#x2F;em[last()]&#x2F;text()&#39;)[3]</span><br><span class="line">    weibo_item[&#39;text&#39;]&#x3D;&#39; &#39;.join(weibo.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;WB_text&quot;]&#x2F;text()&#39;))</span><br><span class="line">    weibo_item[&#39;image_src&#39;]&#x3D;weibo.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;media_box&quot;]&#x2F;&#x2F;img&#x2F;@src&#39;)</span><br><span class="line">    weibo_item[&#39;time&#39;]&#x3D;weibo.xpath(&#39;.&#x2F;&#x2F;div[contains(@class,&quot;WB_from&quot;)]&#x2F;a[position()&#x3D;1]&#x2F;text()&#39;)[0]</span><br><span class="line">    weibo_item[&#39;phone&#39;]&#x3D;weibo.xpath(&#39;.&#x2F;&#x2F;div[contains(@class,&quot;WB_from&quot;)]&#x2F;a[position()&#x3D;2]&#x2F;text()&#39;)[0]</span><br><span class="line">    yield weibo_item</span><br></pre></td></tr></table></figure>

<p><strong>提示：可以将网页的HTML保存进项目目录，正则表达式测试工具、XPath测试工具来测试所编写的选择器，甚至可以写一段测试代码，查看选择器返回后的数据及其类型</strong></p>
<h2 id="4-实现递归的方式"><a href="#4-实现递归的方式" class="headerlink" title="4.实现递归的方式"></a>4.实现递归的方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">class WeibocomSpider(Spider):</span><br><span class="line">    #选取几个微博大V的列表</span><br><span class="line">    start_users &#x3D; [&#39;1003062687827715&#39;, &#39;1004061797270765&#39;, &#39;1005051046193337&#39;, &#39;1004061241148864&#39;]</span><br><span class="line">    </span><br><span class="line">    def start_requests(self):</span><br><span class="line">        #依次抓取各个大V的个人详情，使用parse_user作为回调函数</span><br><span class="line">        for page_id in self.start_users:</span><br><span class="line">            yield Request(self.user_url.format(page_id&#x3D;page_id), callback&#x3D;self.parse_user)</span><br><span class="line">      </span><br><span class="line">    def parse_user(self, response):</span><br><span class="line">        #借由response.url中的page_id，构造访问关注列表、粉丝列表、微博列表的Resquest</span><br><span class="line">        # 关注</span><br><span class="line">        page_id &#x3D; re.search(r&#39;p&#x2F;(\d*)&#x2F;info&#39;,response.url).group(1)</span><br><span class="line">        yield Request(self.follow_url.format(page_id&#x3D;page_id, page&#x3D;1), callback&#x3D;self.parse_follows,</span><br><span class="line">                          meta&#x3D;&#123;&#39;page&#39;: 1, &#39;page_id&#39;: page_id&#125;)</span><br><span class="line">        # 粉丝</span><br><span class="line">        yield Request(self.fan_url.format(page_id&#x3D;page_id, page&#x3D;1), callback&#x3D;self.parse_fans,</span><br><span class="line">                          meta&#x3D;&#123;&#39;page&#39;: 1, &#39;page_id&#39;: page_id&#125;)</span><br><span class="line">        # 微博</span><br><span class="line">        yield Request(self.weibo_url.format(page_id&#x3D;page_id, page&#x3D;1), callback&#x3D;self.parse_weibos,</span><br><span class="line">                          meta&#x3D;&#123;&#39;page&#39;: 1, &#39;page_id&#39;: page_id&#125;)</span><br><span class="line">    </span><br><span class="line">    def getpage_id(self,uid):</span><br><span class="line">        #根据uid(十位)访问获取domid，拼成能访问用户详情页的page_id(十六位)</span><br><span class="line">        response&#x3D;requests.get(&#39;https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;&#123;uid&#125;&#39;.format(uid&#x3D;uid))</span><br><span class="line">        return re.search(r&quot;$CONFIG[&#39;page_id&#39;]&#x3D;&#39;(\d&#123;16&#125;)&#39;;&quot;,response.text)</span><br><span class="line">        </span><br><span class="line">    def parse_follows(self, response):</span><br><span class="line">        #解析被关注的用户的信息</span><br><span class="line">        for follow in follows:</span><br><span class="line">            uid&#x3D;re.search(r&#39;uid&#x3D;(\d&#123;10&#125;)&amp;&#39;,follow.xpath(&#39;.&#x2F;&#x2F;@action-data&#39;)[0]).group(1)</span><br><span class="line">            page_id&#x3D;self.getpage_id(uid)#被关注者的page_id</span><br><span class="line">            yield Request(self.user_url.format(page_id&#x3D;page_id),callback&#x3D;self.parse_user)</span><br><span class="line">        #下一页</span><br><span class="line">        page&#x3D;int(re.search(r&#39;page&#x3D;(\d)&#39;,response.url).group(1))+1</span><br><span class="line">        if page&lt;&#x3D;5:</span><br><span class="line">            yield Request(self.follow_url.format(page_id&#x3D;page_id,page&#x3D;page),</span><br><span class="line">                    callback&#x3D;self.parse_follows, meta&#x3D;&#123;&#39;page&#39;: page, &#39;page_id&#39;: page_id&#125;)</span><br><span class="line">        </span><br><span class="line">    def parse_fans(self, response):</span><br><span class="line">        #解析粉丝的用户信息</span><br><span class="line">        for fan in fans:</span><br><span class="line">            uid&#x3D;re.search(r&#39;uid&#x3D;(\d&#123;10&#125;)&amp;&#39;,fan.xpath(&#39;.&#x2F;&#x2F;@action-data&#39;)[0]).group(1)</span><br><span class="line">            page_id&#x3D;self.getpage_id(uid)#被关注者的page_id</span><br><span class="line">            yield Request(self.user_url.format(page_id&#x3D;page_id),callback&#x3D;self.parse_user)</span><br><span class="line">        #下一页粉丝</span><br><span class="line">        page&#x3D;int(re.search(r&#39;page&#x3D;(\d)&#39;,response.url).group(1))+1</span><br><span class="line">        if page&lt;&#x3D;5:</span><br><span class="line">            yield Request(self.fan_url.format(page_id&#x3D;page_id,page&#x3D;page),</span><br><span class="line">                    callback&#x3D;self.parse_fans, meta&#x3D;&#123;&#39;page&#39;: page, &#39;page_id&#39;: page_id&#125;)</span><br><span class="line"></span><br><span class="line">    def parse_weibos(self, response):</span><br><span class="line">        #下一页微博</span><br><span class="line">        page_id&#x3D;re.search(r&#39;p&#x2F;(\d&#123;16&#125;)&#x2F;home?&#39;,response.url).group(1)</span><br><span class="line">        page&#x3D;int(re.search(r&#39;page&#x3D;(\d)&#39;,response.url).gruop(1))+1</span><br><span class="line">        yield Request(self.weibo_url.format(page_id&#x3D;page_id, page&#x3D;page), callback&#x3D;self.parse_weibos,</span><br><span class="line">                          meta&#x3D;&#123;&#39;page_id&#39;: page_id, &#39;page&#39;: page&#125;)</span><br></pre></td></tr></table></figure>
<p>通过访问微博大V的用户详情页，生成初始Request并用parse_user作为回调函数。parse_user解析用户详情页，通过response.url中的page_id构造访问关注列表、粉丝列表、微博列表的Resquest。在parse_fans、parse_follows、parse_weibos中，通过response.url中的page_id和page构造下一页的Request。</p>
<h2 id="5-将Items保存到MongoDB"><a href="#5-将Items保存到MongoDB" class="headerlink" title="5.将Items保存到MongoDB"></a>5.将Items保存到MongoDB</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class MongoPipeline(object):</span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri &#x3D; mongo_uri</span><br><span class="line">        self.mongo_db &#x3D; mongo_db</span><br><span class="line">    </span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri&#x3D;crawler.settings.get(&#39;MONGO_URI&#39;),</span><br><span class="line">            mongo_db&#x3D;crawler.settings.get(&#39;MONGO_DATABASE&#39;)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client &#x3D; pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db &#x3D; self.client[self.mongo_db]</span><br><span class="line">        #为两个Collection创建索引，索引字段是id</span><br><span class="line">        self.db[UserItem.collection].create_index([(&#39;id&#39;, pymongo.ASCENDING)])</span><br><span class="line">        self.db[WeiboItem.collection].create_index([(&#39;id&#39;, pymongo.ASCENDING)])</span><br><span class="line">    </span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.client.close()</span><br><span class="line">    </span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if isinstance(item, UserItem) or isinstance(item, WeiboItem):#isinstance判断两个类型是否相同</span><br><span class="line">            self.db[item.collection].update(&#123;&#39;id&#39;:item.get(&#39;id&#39;)&#125;, &#123;&#39;$set&#39;:item&#125;, True)</span><br><span class="line">            #$set 操作符，如果爬取到重复的数据即可对数据进行更新，同时不会删除已存在的字段 </span><br><span class="line">            #参数设置为True ，如果数据不存在则插入数据。这样我们就可以做到数据存在即更新，数据不存在即插入，从而获得去重的效果</span><br><span class="line">        if isinstance(item, UserRelationItem):</span><br><span class="line">            self.db[item.collection].update(</span><br><span class="line">                &#123;&#39;page_id&#39;: item.get(&#39;page_id&#39;)&#125;,</span><br><span class="line">                &#123;&#39;$addToSet&#39;:#$addToSet操作符可以向列表类型的字段插入数据同时去重，它的值就是需要操作的字段名称</span><br><span class="line">                    &#123;</span><br><span class="line">                        &#39;follows&#39;: &#123;&#39;$each&#39;: item[&#39;follows&#39;]&#125;,</span><br><span class="line">                        #$each操作符对需要插入的列表数据进行了遍历，以逐条插入用户的关注或粉丝数据到指定的字段</span><br><span class="line">                        &#39;fans&#39;: &#123;&#39;$each&#39;: item[&#39;fans&#39;]&#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;, True)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<p>最重要的是process_item方法，Item Pipeline会默认调用该方法对Items进行处理。该process_item方法所做的是，如果item是UserItem和WeiboItem则更新相应的Collection，如果是UserRelationItem则向users Collection插入粉丝列表和关注列表。<br>如何对MongoDB进行操作，我有很多操作符都不是很懂，没见过也不会用。</p>
<h2 id="6-对接Cookies池"><a href="#6-对接Cookies池" class="headerlink" title="6.对接Cookies池"></a>6.对接Cookies池</h2><p>如何用Python模拟登录微博，如何构建Cookies池，在别的博文里有提到。这里我们需要从Redis数据库中随机提取cookies，并将Request的cookies更换为Cookies池的cookies。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class CookiesMiddleware():</span><br><span class="line">    def __init__(self,host,port,password):</span><br><span class="line">        self.logger &#x3D; logging.getLogger(__name__)</span><br><span class="line">        self.cookies_db &#x3D; redis.StrictRedis(host&#x3D;host, port&#x3D;port, password&#x3D;password, decode_responses&#x3D;True)</span><br><span class="line">    </span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        settings &#x3D; crawler.settings</span><br><span class="line">        return cls(</span><br><span class="line">                host &#x3D; settings.get(&#39;REDIS_HOST&#39;),</span><br><span class="line">                port &#x3D; settings.get(&#39;REDIS_PORT&#39;),</span><br><span class="line">                password &#x3D; settings.get(&#39;REDIS_PASSWORD&#39;)</span><br><span class="line">               )</span><br><span class="line">   </span><br><span class="line">    def get_random_cookies(self):</span><br><span class="line">        cookies &#x3D; self.cookies_db.hvals(&#39;cookies:weibo&#39;)</span><br><span class="line">        return random.choice(cookies)</span><br><span class="line">    </span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        self.logger.debug(&#39;正在获取Cookies&#39;)</span><br><span class="line">        cookies &#x3D; self.get_random_cookies()#获取到的cookies为字符串类型</span><br><span class="line">        cookies &#x3D; json.loads(cookies)</span><br><span class="line">        if cookies:</span><br><span class="line">            request.cookies &#x3D; cookies</span><br><span class="line">            self.logger.debug(&#39;使用Cookies &#39; , cookies)</span><br></pre></td></tr></table></figure>
<p>process_request是Downloader Middleware的核心方法之一，另外两个是process_response和process_exception。</p>
<h2 id="7-settings-py打开pipeline和middleware"><a href="#7-settings-py打开pipeline和middleware" class="headerlink" title="7.settings.py打开pipeline和middleware"></a>7.settings.py打开pipeline和middleware</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES &#x3D; &#123;</span><br><span class="line">    #&#39;weiboScrapy.middlewares.WeiboscrapyDownloaderMiddleware&#39;: 543,</span><br><span class="line">    &#39;weiboScrapy.middlewares.CookiesMiddleware&#39;: 550,</span><br><span class="line">    #&#39;weiboScrapy.middlewares.ProxyMiddleware&#39;: 555,</span><br><span class="line">&#125;</span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">    #&#39;weiboScrapy.pipelines.WeiboscrapyPipeline&#39;: 300,</span><br><span class="line">    #&#39;weiboScrapy.pipelines.WeiboPipeline&#39;: 301,</span><br><span class="line">    &#39;weiboScrapy.pipelines.MongoPipeline&#39;: 302,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MONGO_URI &#x3D; &#39;localhost&#39;</span><br><span class="line">MONGO_DATABASE &#x3D; &#39;weibo&#39;</span><br><span class="line"></span><br><span class="line">REDIS_HOST &#x3D; &#39;127.0.0.1&#39;</span><br><span class="line">REDIS_PORT &#x3D; &#39;6379&#39;</span><br><span class="line">REDIS_PASSWORD &#x3D; None</span><br></pre></td></tr></table></figure>

<h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><p><strong>爬虫时遇到的难点：</strong></p>
<ol>
<li>必须要登陆才能看到关注信息；</li>
<li>只可以查看前五页的粉丝列表关注者列表；</li>
<li>用户主页微博列表是动态生成的，那些动态加载的微博在Network中再搜索不出，但是划到最后是可以找到分页的。</li>
<li>URL所用到的page_id要比uid多几位数字，如100406、100505、100306，即“domid”，应该是用来区分用户类型的。如果URL中用错了或者省略domid，会被重定向到主页。因此在提取关注者、粉丝信息时，还需要知道domid。</li>
<li>提取微博文章时，如果文本过长会被折叠，显示“展开全文”链接才会显示全文。点赞数、转发数、评论数是实时变动的，从网页中提取到的永远会滞后。</li>
<li>登录时，账户默认开启了登录保护（新注册的账号也一样），要求你进行短信验证或私信验证。</li>
</ol>
<p><strong>解决策略：</strong><br>（1）微博模拟登陆，构建Cookies池<br>（2）由于是系统限制只能搜集前五页的粉丝列表和关注列表，因此就只能按照其规则来。<br>（3）动态生成，是因为执行JavaScript后再向后台发送了Ajax请求，浏览器拿到数据后再进一步渲染出来的。所以我们需要用程序模拟这些Ajax请求，提取我们想要的数据。<br>（4）对于缺少domid的uid，可以先通过<a target="_blank" rel="noopener" href="http://weibo.com/u/%7Buid%7D%E8%AE%BF%E9%97%AE%E4%B8%BB%E9%A1%B5%EF%BC%8C%E4%BB%8E%E4%B8%BB%E9%A1%B5%E7%9A%84%E7%BD%91%E9%A1%B5%E4%B8%AD%E8%8E%B7%E5%8F%96domid%E3%80%82">http://weibo.com/u/{uid}访问主页，从主页的网页中获取domid。</a><br>（6）输入用户名和密码之后，会弹出短信验证或私信验证（默认开启的登录保护），有时又还会弹出验证码。短信验证和私信验证通过寻常的反爬虫手段无法解决（起码我的能力有限想不到解决办法），但是我找到了可以绕过登录保护的办法，就是先登录新浪邮箱，再从新浪邮箱跳转到微博，这样可以直接登录。**<em>我用来做爬虫的微博账号都是通过新浪邮箱注册的，所以这种方法仅针对通过新浪邮箱的账号和密码来登录微博的情况**</em></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/09/27/%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE/" data-id="ckfkv4duq0001lku7c8ct9wbc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/09/27/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/09/27/%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE/">用Scrapy框架爬取微博数据</a>
          </li>
        
          <li>
            <a href="/2020/09/27/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 左菲的博客<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>